{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4da9e2d-daed-4a16-bfff-fb5f087d4df2",
   "metadata": {},
   "source": [
    "# Simulation Based Inference For NeuroScience: The BOLD signal <a class=\"anchor\" id=\"top\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d3990e-103d-4b2a-9a10-581cb6e9a6f3",
   "metadata": {},
   "source": [
    "## Table Of Content:\n",
    "* [Setup](#set-up)\n",
    "* [Train a Density Estimator](#density-estimator)\n",
    "* [Creating X with \"old betas\"](#creating-X-old)\n",
    "* [Creating X with \"new betas\"](#creating-X-new)\n",
    "* [Loading Simulation Results](#simulation)\n",
    "* [Training the Neural Network](#training)\n",
    "* [Validating Results & Saving Plots](#validation)\n",
    "    - [Setup parameters for testing](#setup-testing-params)\n",
    "    - [Load the models and the testing data](#load-test-data)\n",
    "    - [Running vs Loading Test](#run-vs-load)\n",
    "    - [Run Tests](#run-tests)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6330678-2999-4b88-8d0b-ea93e8c14fd6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Setup<a class=\"anchor\" id=\"set-up\"></a>\n",
    "\n",
    "First import the needed libraries (and set current working directory if needed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e728d79-1e16-4f04-b721-02b4f97844fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "\n",
    "# General libraries:\n",
    "import numpy as np\n",
    "import pylab as plt\n",
    "import argparse \n",
    "import torch \n",
    "\n",
    "from sbi.inference import SNPE, SNLE, SNRE\n",
    "\n",
    "# For plotting:\n",
    "from sbi.analysis import pairplot, conditional_pairplot\n",
    "from utils import marginal_correlation, percentile_distribution, plot_layer_combinations, accuracy_per_layer\n",
    "\n",
    "# Functions:\n",
    "from train import train\n",
    "from test import test_posterior, return_single_results, print_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4af39ac3-bbfa-4763-b776-6159a632a603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If needed, before starting, change the current working directory by uncommenting and inserting the right path:\n",
    "# os.chdir(\"/home/coder/projects/lorenz_sbi\")\n",
    "\n",
    "# Get the path of the current working directory: \n",
    "cwd_path = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e07450",
   "metadata": {},
   "source": [
    "## Train a density estimator<a class=\"anchor\" id=\"density-estimator\"></a>\n",
    "\n",
    "When training the model, be aware the change the second line of the code to the desired training set:\n",
    "\n",
    "\"parser.add_argument(\"--data\", type=str, default=cwd_path + \"/data/X_train_01.npy\", help=\"Path to the data file.\")\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e75d21ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a density estimator.\n",
    "# If desired, change the number of threads to a higher number (by default it's 1).\n",
    "parser = argparse.ArgumentParser(description=\"Train a density estimator.\")\n",
    "parser.add_argument(\"--data\", type=str, default=cwd_path + \"/data/X_train_01.npy\", help=\"Path to the data file.\")\n",
    "parser.add_argument(\"--method\", type=str, default=\"SNPE\", help=\"Inference method.\")\n",
    "parser.add_argument(\"--density_estimator\", type=str, default=\"maf\", help=\"Density estimator.\")\n",
    "parser.add_argument(\"--num_threads\", type=int, default=1, help=\"Number of threads.\")\n",
    "parser.add_argument(\"--device\", type=str, default=\"cpu\", help=\"Device.\")\n",
    "\n",
    "args, unknown = parser.parse_known_args()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f74a4a",
   "metadata": {},
   "source": [
    "## Creating X with \"old betas\"<a class=\"anchor\" id=\"creating-X-old\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3bed2ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does the length match between the parameters and the beta value (returns True if this is true)? True\n",
      "The shape of X is:  (11520, 12)\n",
      "The first row/simulation in X is:  \n",
      " [49.20419452  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.9181171   0.69864174  0.71237282  0.71968232]\n"
     ]
    }
   ],
   "source": [
    "# Read in beta files (input to our NN).\n",
    "# First read in 1 file:\n",
    "betas = np.load(cwd_path + \"/data/Betas_01.npy\")\n",
    "\n",
    "# Append the rest of the beta batches to this file:\n",
    "for number in range(2, 11):\n",
    "    #print(number)\n",
    "    if number <= 9:\n",
    "        beta_files = np.load(cwd_path + \"/data/Betas_0\" + str(number) + \".npy\")\n",
    "        betas = np.concatenate((betas, beta_files))\n",
    "    else:\n",
    "        beta_files = np.load(cwd_path + \"/data/Betas_10.npy\")\n",
    "        betas = np.concatenate((betas, beta_files))\n",
    "\n",
    "# Read in our parameters.\n",
    "total_combinations = np.load(cwd_path + \"/data/total_combinations.npy\")\n",
    "\n",
    "\n",
    "# Check whether the number of rows match between our parameters and beta values.\n",
    "print(\"Does the length match between the parameters and the beta value (returns True if this is true)?\", len(total_combinations) == len(betas))\n",
    "\n",
    "# Concatenate both together:\n",
    "X = np.concatenate((total_combinations, betas), axis = 1)\n",
    "\n",
    "print(\"The shape of X is: \", X.shape)\n",
    "print(\"The first row/simulation in X is: \", \"\\n\", X[0])\n",
    "\n",
    "np.save(cwd_path + \"/data/X.npy\", X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55d1e8e",
   "metadata": {},
   "source": [
    "## Creating X with \"new betas\"<a class=\"anchor\" id=\"creating-X-new\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1bb1b31b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does the length match between the parameters and the beta value (returns True if this is true)? True\n",
      "The shape of X is:  (11520, 12)\n",
      "The first row/simulation in X is:  \n",
      " [49.20419452  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.72805587  0.80093876  0.75194262  0.74366456]\n"
     ]
    }
   ],
   "source": [
    "# Read in beta files (input to our NN).\n",
    "# First read in 1 file:\n",
    "new_betas = np.load(cwd_path + \"/data/Betas_inh_exc_summed_01.npy\")\n",
    "\n",
    "# Append the rest of the beta batches to this file:\n",
    "for number in range(2, 11):\n",
    "    #print(number)\n",
    "    if number <= 9:\n",
    "        new_beta_files = np.load(cwd_path + \"/data/Betas_inh_exc_summed_0\" + str(number) + \".npy\")\n",
    "        new_betas = np.concatenate((new_betas, new_beta_files))\n",
    "    else:\n",
    "        new_beta_files = np.load(cwd_path + \"/data/Betas_inh_exc_summed_10.npy\")\n",
    "        new_betas = np.concatenate((new_betas, new_beta_files))\n",
    "\n",
    "# Read in our parameters.\n",
    "total_combinations = np.load(cwd_path + \"/data/total_combinations.npy\")\n",
    "\n",
    "# Check whether the number of rows match between our parameters and beta values.\n",
    "print(\"Does the length match between the parameters and the beta value (returns True if this is true)?\", len(total_combinations) == len(new_betas))\n",
    "\n",
    "# Concatenate both together:\n",
    "X_with_new_betas = np.concatenate((total_combinations, new_betas), axis = 1)\n",
    "\n",
    "print(\"The shape of X is: \", X_with_new_betas.shape)\n",
    "print(\"The first row/simulation in X is: \", \"\\n\", X_with_new_betas[0])\n",
    "\n",
    "np.save(cwd_path + \"/data/X_with_new_betas.npy\", X_with_new_betas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a65b248-946e-4a4b-9f9c-6ea441ab8c8f",
   "metadata": {},
   "source": [
    "<div style=\"visibility:hidden\">d</div>\n",
    "<a href=\"#top\" style=\"position: sticky; right: 0;   text-decoration: none; margin: .5rem; padding: 1rem; font-family: sans-serif; color: #fff; background: #123; border-radius: 100px; white-space: nowrap;\">Back to Top &#8593;</a>\n",
    "<div style=\"visibility:hidden\">d</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf2a7b5-6c6e-4478-b8f8-007091d4289d",
   "metadata": {},
   "source": [
    "## Loading Simulation Results<a class=\"anchor\" id=\"simulation\"></a>\n",
    "\n",
    "As shuffle is not deterministic (as np.random.seed() does not work as intended), we will save our X_train and X_test instead.\n",
    "\n",
    "We will make 3 models for the \"old betas\" called posterior_01, posterior_02 and posterior_03. Their corresponding training and test data (present in the data map) are respectively: \n",
    "-\tX_train_01 with X_test_01\n",
    "-\tX_train_02 with X_test_02\n",
    "-\tX_train_03 with X_test_03\n",
    "\n",
    "So as example: the model \"posterior_01\" is trained with X_train_01 (so you need to test this model with X_test_01). Same for the other 2 models but then with the 02 and 03 files.\n",
    "\n",
    "We will also make 3 models for the \"new betas\" called posterior_with_new_betas_01, posterior_with_new_betas_02 and posterior_with_new_betas_03. Their corresponding training and test data (present in the data map) are respectively: \n",
    "-\tX_train_new_betas_01 with X_test_new_betas_01\n",
    "-\tX_train_new_betas_02 with X_test_new_betas_02\n",
    "-\tX_train_new_betas_03 with X_test_new_betas_03\n",
    "\n",
    "Again as example: the model \"posterior_with_new_betas_01\" is trained with X_train_new_betas_01 (so you need to test this model with X_test_new_betas_01). Same for the other 2 models but then with the 02 and 03 files.\n",
    "\n",
    "The reason to train multiple models is to avoid that by chance a better train/test set is generated which can lead to optimistic/worse results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa87b568-4118-493c-b7e9-cba3db39b7a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11520, 12)\n",
      "[49.20419452  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.9181171   0.69864174  0.71237282  0.71968232]\n"
     ]
    }
   ],
   "source": [
    "# Change the line below to the desired X that you want to import.\n",
    "# X.npy in data map contains the \"old betas\" and X_with_new_betas.npy contains the \"new betas\".\n",
    "X = np.load(cwd_path + \"/data/X.npy\")\n",
    "print(X.shape)\n",
    "print(X[0])\n",
    "\n",
    "# # Split this matrix into train and test data.\n",
    "\n",
    "# # Shuffle the X to make sure different combinations end up in the train and test sets.\n",
    "# np.random.shuffle(X)\n",
    "\n",
    "# # Take 10% of the data for test (there are 11520 simulations in X, 10% is 1152 which goes to test, the rest goes to X_train).\n",
    "# X_train = X[:10368, :]\n",
    "# X_test = X[10368:, :]\n",
    "\n",
    "# # Save this train and test set; specify the files you want the train and test data to be saved to.\n",
    "# np.save(cwd_path + \"/data/X_train_[...].npy\", X_train)\n",
    "# np.save(cwd_path + \"/data/X_test_[...].npy\", X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4b9a8d-93ab-43c0-bdfc-50aca85e2187",
   "metadata": {},
   "source": [
    "## Training the Neural Network <a class=\"anchor\" id=\"training\"></a>\n",
    "\n",
    "There is a pretrained posterior present in the models map. If you want to retrain it, uncomment the block of code below before running it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4407948a-0310-474d-aa31-30de5523615c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the 2 lines below to load the desired train and test set (here I loaded X_train_01 and X_test_01)\n",
    "# which is with the \"old betas\" (first run from the triplo).\n",
    "X_train_01 = np.load(args.data, allow_pickle=True)\n",
    "X_test_01 = np.load(cwd_path + \"/data/X_test_01.npy\")\n",
    "\n",
    "# # Establish how many simulations are in the data \n",
    "# num_simulations = X_train_01.shape[0]\n",
    "\n",
    "# # Seperate simulation parameters and summary statistics\n",
    "# params = X_train_01[:, -4:]\n",
    "# stats  = X_train_01[:, :-4]\n",
    "\n",
    "# # When working with Torch, the matrix has to be parsed to a Torch object \n",
    "# theta = torch.from_numpy(params).float()\n",
    "# x = torch.from_numpy(stats).float()\n",
    "\n",
    "# # Train the posterior with all the arguments needed \n",
    "# posterior = train(num_simulations,\n",
    "#                     x,\n",
    "#                     theta,\n",
    "#                     num_threads         = args.num_threads,\n",
    "#                     method              = args.method,\n",
    "#                     device              = args.device,\n",
    "#                     density_estimator   = args.density_estimator\n",
    "#                     )\n",
    "\n",
    "# # Save posterior (intermediate result); specify the files you want the train and test data to be saved to.\n",
    "# torch.save(posterior, cwd_path + \"/models/posterior_[...].pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270eb615-8e16-493a-9905-7d8c5497bc01",
   "metadata": {},
   "source": [
    "<div style=\"visibility:hidden\">d</div>\n",
    "<a href=\"#top\" style=\"position: sticky; right: 0;   text-decoration: none; margin: .5rem; padding: 1rem; font-family: sans-serif; color: #fff; background: #123; border-radius: 100px; white-space: nowrap;\">Back to Top &#8593;</a>\n",
    "<div style=\"visibility:hidden\">d</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45087be-d8dc-4520-9ce7-5035813a611b",
   "metadata": {},
   "source": [
    "## Validating Results & Saving Plots <a class=\"anchor\" id=\"validation\"></a>\n",
    "\n",
    "This section is designed to test all the models that were trained on a generated dataset. It first load the posteriors that were generated previously, the testing data and run the test_posterior() method on each of them. \n",
    "The testing parameters are the following: \n",
    "\n",
    "- threshold: A value between 0 and 1, which establishes the range in which a predicted value for a population is considered to be close enough to the actual value\n",
    "- tolerance: This parameter extends the range to take into account that for layers with value of 0, the range would be 0 and the posterior sampling always return a non-zero value for each population input. \n",
    "- activation_threshold: At what value a population is considered activated\n",
    "- num_samples: How many samples should be sampled from the posterior at each test (This has the largest effect on testing speed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c2ea4a-a60d-49bd-9c76-4156c73599fc",
   "metadata": {},
   "source": [
    "### Setup parameters for testing  <a class=\"anchor\" id=\"setup-testing-params\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0cf82ecb-085a-4967-b155-27c3d87feb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.3\n",
    "tolerance = 1.0\n",
    "activation_threshold = 10.0\n",
    "num_samples = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1bea0d-154a-4bea-bd81-2f33e9e762c8",
   "metadata": {},
   "source": [
    "When generating posterior models, they are all created with a common prefix. We defined here which one to use for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9657ea26-b8a8-4f84-8472-026d71c6a50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# posterior_prefix = \"posterior_\"\n",
    "# posterior_prefix = \"posterior_with_new_betas_\"\n",
    "posterior_prefix = \"posterior_with_all_data_\"\n",
    "\n",
    "# test_mat_prefix = \"X_test_\"\n",
    "# test_mat_prefix = \"X_train_new_betas_\"\n",
    "test_mat_prefix = \"all_data_test_\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2bdbe3-1683-40ab-ae45-26c0fb695790",
   "metadata": {},
   "source": [
    "### Load the models and the testing data  <a class=\"anchor\" id=\"load-test-data\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73040c6-c2df-4d8d-be94-edccac92e472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up arrays \n",
    "#    They will have shape (Number of Models, 2)\n",
    "#    Each row has the name of the posterior and the model (same for testing data)\n",
    "posterior_models = []\n",
    "testing_matrices = []\n",
    "\n",
    "# Walk through model dir and append all models that have the correct prefix \n",
    "for root, dirs, files in os.walk(cwd_path + \"/models/\"):\n",
    "    for name in files:\n",
    "        if posterior_prefix in name:\n",
    "            posterior_models.append([name, torch.load(os.path.join(root, name))])\n",
    "\n",
    "# Idem for testing data\n",
    "for root, dirs, files in os.walk(cwd_path + \"/data/\"):\n",
    "    for name in files:\n",
    "        if test_mat_prefix in name:\n",
    "            testing_matrices.append([name , np.load(os.path.join(root, name))])\n",
    "\n",
    "# Convert to np.arrays             \n",
    "posterior_models = np.array(posterior_models, dtype=object)\n",
    "testing_matrices = np.array(testing_matrices, dtype=object)\n",
    "\n",
    "# Combine into array with each row is the combination of a posterior and the corresponding testing_matrix\n",
    "testing_data = np.array([[posterior_models[i],testing_matrices[i]] for i in range(len(testing_matrices))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c08b6a-0ecc-4022-ae38-5610bbce2ec8",
   "metadata": {},
   "source": [
    "### Running vs Loading Test  <a class=\"anchor\" id=\"run-vs-load\"></a>\n",
    "When running test, the boolean `run_tests` should be true, and the `cur_test_name` variable can be generated. If loading, then the test result object should be passed in `cur_test_name`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bea0efe-b664-4fa2-ac5e-1c3c34162af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## When running test \n",
    "# run_tests = True\n",
    "# cur_test_name = \"test_suite_\" + str(datetime.now())[5:10] + \"_\" + str(datetime.now())[11:-7].replace(\":\",\"_\")\n",
    "\n",
    "## When loading test\n",
    "run_tests = False\n",
    "cur_test_name = \"test_suite_10-26_21_13_56\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2710e9-47dc-4686-8b54-5d8006933c0e",
   "metadata": {},
   "source": [
    "### Run Tests <a class=\"anchor\" id=\"run-tests\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ea7d90fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tests results:  test_suite_10-26_21_13_56\n",
      "Testing Parameters: \n",
      "\t Posterior Prefix: posterior_with_all_data_\n",
      "\t Testing Prefix:   all_data_test_\n",
      "\t Threshold: 0.3\n",
      "\t Tolerance: 1.0\n",
      "\t Activation Threshold: 10.0\n",
      "\t Number of Samples: 500\n"
     ]
    }
   ],
   "source": [
    "test_results_arr = []\n",
    "\n",
    "if run_tests:\n",
    "    \n",
    "    print(\"Running tests for models: \")\n",
    "    for posterior in posterior_models:\n",
    "        print(posterior[0])\n",
    "    print()\n",
    "    \n",
    "    for (posterior, test_matrix) in testing_data:\n",
    "        test_results_arr.append(\n",
    "            test_posterior(\n",
    "                posterior[1], \n",
    "                test_matrix[1], \n",
    "                num_samples = num_samples, \n",
    "                threshold = threshold, \n",
    "                tolerance = tolerance,\n",
    "                activation_threshold = activation_threshold,\n",
    "                posterior_name = posterior[0])\n",
    "        )\n",
    "    \n",
    "    \n",
    "    # Save test results\n",
    "    np.save(os.path.join(cwd_path,\"test_results\",cur_test_name), test_results_arr)\n",
    "    \n",
    "    # Save test params\n",
    "    with open(os.path.join(cwd_path, 'test_results', cur_test_name + '_params.txt'), 'w') as f:\n",
    "        f.write(\"Posterior Prefix:     \" + posterior_prefix + \"\\n\")\n",
    "        f.write(\"Testing Prefix:       \" + test_mat_prefix + \"\\n\")\n",
    "        f.write(\"Threshold:            \" + str(threshold) + \"\\n\")\n",
    "        f.write(\"Tolerance:            \" + str(tolerance) + \"\\n\")\n",
    "        f.write(\"Activation Threshold: \" + str(activation_threshold) + \"\\n\")\n",
    "        f.write(\"Number of Samples:    \" + str(num_samples) + \"\\n\")\n",
    "    \n",
    "else:\n",
    "    print(\"Loading tests results: \", cur_test_name)\n",
    "    print(\"Testing Parameters: \")\n",
    "    with open(os.path.join(cwd_path,\"plot_images\",cur_test_name,\"test_params.txt\")) as f:\n",
    "        for line in f.readlines():\n",
    "            print(\"\\t\",line[:-1])\n",
    "    \n",
    "    test_results_arr = np.load(os.path.join(cwd_path,\"test_results\",cur_test_name + \".npy\"), allow_pickle=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9d1820-98f2-4491-bf1e-6c98a2198c2c",
   "metadata": {},
   "source": [
    "### Generate plots "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "01bcc93c-1841-4d8a-8885-c37f6cfcd2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_test_suite_dir = os.path.join(cwd_path, 'plot_images', cur_test_name)\n",
    "\n",
    "try:\n",
    "    os.makedirs(cur_test_suite_dir)\n",
    "except FileExistsError:\n",
    "    pass\n",
    "\n",
    "print(\"Saving plots...\")\n",
    "\n",
    "# Plot average accuracy accross each model \n",
    "fig00 , _ = plot_average_accuracy(test_results_arr)\n",
    "fig00.savefig(os.path.join(cur_test_suite_dir, 'average_accuracy'))\n",
    "plt.close(fig00)\n",
    "\n",
    "for test_indx in tqdm(range(len(test_results_arr)), unit=\" model\", colour=\"green\", file=sys.stdout):\n",
    "    \n",
    "    test_result = test_results_arr[test_indx]\n",
    "    \n",
    "    # Get the name of the posterior related to the test results and generate a dir \n",
    "    cur_test_dir = os.path.join(cwd_path, 'plot_images', cur_test_suite_dir, os.path.splitext(combi[test_indx][0][0])[0])\n",
    "    \n",
    "    try:\n",
    "        os.mkdir(cur_test_dir)\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "    \n",
    "    # Generate plots\n",
    "    fig1    = plot_confusion_matrices_and_balanced_accuracies(test_result)\n",
    "    fig2    = plot_confusion_matrices_and_balanced_accuracies_combinations(test_result)\n",
    "    fig3, _ = plot_layer_combinations(combi[test_indx][1][1])\n",
    "    \n",
    "    # Save figures\n",
    "    fig1.savefig(os.path.join(cur_test_dir, 'accuracy'))  \n",
    "    fig2.savefig(os.path.join(cur_test_dir, 'accuracy_combinations'))  \n",
    "    fig3.savefig(os.path.join(cur_test_dir, 'activation_population_inputs'))\n",
    "    \n",
    "    # Close figures to prevent them to show in std.out (jupyter cell)\n",
    "    plt.close(fig1)\n",
    "    plt.close(fig2)\n",
    "    plt.close(fig3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294bc382-f4ca-4295-b91f-d60283f809f6",
   "metadata": {},
   "source": [
    "<div style=\"visibility:hidden\">d</div>\n",
    "<a href=\"#top\" style=\"position: sticky; right: 0;   text-decoration: none; margin: .5rem; padding: 1rem; font-family: sans-serif; color: #fff; background: #123; border-radius: 100px; white-space: nowrap;\">Back to Top &#8593;</a>\n",
    "<div style=\"visibility:hidden\">d</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:sbi_env]",
   "language": "python",
   "name": "conda-env-sbi_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
