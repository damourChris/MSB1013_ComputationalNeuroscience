{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4da9e2d-daed-4a16-bfff-fb5f087d4df2",
   "metadata": {},
   "source": [
    "# Simulation Based Inference For NeuroScience: The BOLD signal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d3990e-103d-4b2a-9a10-581cb6e9a6f3",
   "metadata": {},
   "source": [
    "## Table Of Content:\n",
    "* [Setup](#set-up)\n",
    "* [Train a Density Estimator](#density-estimator)\n",
    "* [Creating X](#creating-X)\n",
    "* [Loading Simulation Results](#simulation)\n",
    "* [Training the Neural Network](#training)\n",
    "* [Validating Results & Saving Plots](#validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6330678-2999-4b88-8d0b-ea93e8c14fd6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Setup<a class=\"anchor\" id=\"set-up\"></a>\n",
    "\n",
    "First import the needed libraries (and set current working directory if needed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e728d79-1e16-4f04-b721-02b4f97844fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If needed, before starting, change the current working directory by uncommenting and inserting the right path:\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "# os.chdir(\"/home/coder/projects/lorenz_sbi\")\n",
    "\n",
    "# General libraries:\n",
    "import numpy as np\n",
    "import pylab as plt\n",
    "import argparse \n",
    "import torch \n",
    "\n",
    "from sbi.inference import SNPE, SNLE, SNRE\n",
    "\n",
    "# For plotting:\n",
    "from sbi.analysis import pairplot, conditional_pairplot\n",
    "from utils import marginal_correlation, percentile_distribution, plot_layer_combinations, accuracy_per_layer\n",
    "\n",
    "# Functions:\n",
    "from train import train\n",
    "from test import test_posterior, return_single_results, print_stats\n",
    "# Get the path of the current working directory: \n",
    "cwd_path = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e07450",
   "metadata": {},
   "source": [
    "## Train a density estimator<a class=\"anchor\" id=\"density-estimator\"></a>\n",
    "\n",
    "When training the model, be aware the change the second line of the code to the desired training set:\n",
    "\n",
    "\"parser.add_argument(\"--data\", type=str, default=cwd_path + \"/data/X_train_01.npy\", help=\"Path to the data file.\")\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e75d21ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a density estimator.\n",
    "# If desired, change the number of threads to a higher number (by default it's 1).\n",
    "parser = argparse.ArgumentParser(description=\"Train a density estimator.\")\n",
    "parser.add_argument(\"--data\", type=str, default=cwd_path + \"/data/X_train_01.npy\", help=\"Path to the data file.\")\n",
    "parser.add_argument(\"--method\", type=str, default=\"SNPE\", help=\"Inference method.\")\n",
    "parser.add_argument(\"--density_estimator\", type=str, default=\"maf\", help=\"Density estimator.\")\n",
    "parser.add_argument(\"--num_threads\", type=int, default=1, help=\"Number of threads.\")\n",
    "parser.add_argument(\"--device\", type=str, default=\"cpu\", help=\"Device.\")\n",
    "\n",
    "args, unknown = parser.parse_known_args()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f74a4a",
   "metadata": {},
   "source": [
    "## Creating X with \"old betas\"<a class=\"anchor\" id=\"creating-X\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3bed2ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does the length match between the parameters and the beta value (returns True if this is true)? True\n",
      "The shape of X is:  (11520, 12)\n",
      "The first row/simulation in X is:  \n",
      " [49.20419452  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.9181171   0.69864174  0.71237282  0.71968232]\n"
     ]
    }
   ],
   "source": [
    "# Read in beta files (input to our NN).\n",
    "# First read in 1 file:\n",
    "betas = np.load(cwd_path + \"/data/Betas_01.npy\")\n",
    "\n",
    "# Append the rest of the beta batches to this file:\n",
    "for number in range(2, 11):\n",
    "    #print(number)\n",
    "    if number <= 9:\n",
    "        beta_files = np.load(cwd_path + \"/data/Betas_0\" + str(number) + \".npy\")\n",
    "        betas = np.concatenate((betas, beta_files))\n",
    "    else:\n",
    "        beta_files = np.load(cwd_path + \"/data/Betas_10.npy\")\n",
    "        betas = np.concatenate((betas, beta_files))\n",
    "\n",
    "# Read in our parameters.\n",
    "total_combinations = np.load(cwd_path + \"/data/total_combinations.npy\")\n",
    "\n",
    "\n",
    "# Check whether the number of rows match between our parameters and beta values.\n",
    "print(\"Does the length match between the parameters and the beta value (returns True if this is true)?\", len(total_combinations) == len(betas))\n",
    "\n",
    "# Concatenate both together:\n",
    "X = np.concatenate((total_combinations, betas), axis = 1)\n",
    "\n",
    "print(\"The shape of X is: \", X.shape)\n",
    "print(\"The first row/simulation in X is: \", \"\\n\", X[0])\n",
    "\n",
    "np.save(cwd_path + \"/data/X.npy\", X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55d1e8e",
   "metadata": {},
   "source": [
    "## Creating X with \"new betas\"<a class=\"anchor\" id=\"creating-X\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1bb1b31b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does the length match between the parameters and the beta value (returns True if this is true)? True\n",
      "The shape of X is:  (11520, 12)\n",
      "The first row/simulation in X is:  \n",
      " [49.20419452  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.72805587  0.80093876  0.75194262  0.74366456]\n"
     ]
    }
   ],
   "source": [
    "# Read in beta files (input to our NN).\n",
    "# First read in 1 file:\n",
    "new_betas = np.load(cwd_path + \"/data/Betas_inh_exc_summed_01.npy\")\n",
    "\n",
    "# Append the rest of the beta batches to this file:\n",
    "for number in range(2, 11):\n",
    "    #print(number)\n",
    "    if number <= 9:\n",
    "        new_beta_files = np.load(cwd_path + \"/data/Betas_inh_exc_summed_0\" + str(number) + \".npy\")\n",
    "        new_betas = np.concatenate((new_betas, new_beta_files))\n",
    "    else:\n",
    "        new_beta_files = np.load(cwd_path + \"/data/Betas_inh_exc_summed_10.npy\")\n",
    "        new_betas = np.concatenate((new_betas, new_beta_files))\n",
    "\n",
    "# Read in our parameters.\n",
    "total_combinations = np.load(cwd_path + \"/data/total_combinations.npy\")\n",
    "\n",
    "# Check whether the number of rows match between our parameters and beta values.\n",
    "print(\"Does the length match between the parameters and the beta value (returns True if this is true)?\", len(total_combinations) == len(new_betas))\n",
    "\n",
    "# Concatenate both together:\n",
    "X_with_new_betas = np.concatenate((total_combinations, new_betas), axis = 1)\n",
    "\n",
    "print(\"The shape of X is: \", X_with_new_betas.shape)\n",
    "print(\"The first row/simulation in X is: \", \"\\n\", X_with_new_betas[0])\n",
    "\n",
    "np.save(cwd_path + \"/data/X_with_new_betas.npy\", X_with_new_betas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf2a7b5-6c6e-4478-b8f8-007091d4289d",
   "metadata": {},
   "source": [
    "## Loading Simulation Results<a class=\"anchor\" id=\"simulation\"></a>\n",
    "\n",
    "As shuffle is not deterministic (as np.random.seed() does not work as intended), we will save our X_train and X_test instead.\n",
    "\n",
    "We will make 3 models for the \"old betas\" called posterior_01, posterior_02 and posterior_03. Their corresponding training and test data (present in the data map) are respectively: \n",
    "-\tX_train_01 with X_test_01\n",
    "-\tX_train_02 with X_test_02\n",
    "-\tX_train_03 with X_test_03\n",
    "\n",
    "So as example: the model \"posterior_01\" is trained with X_train_01 (so you need to test this model with X_test_01). Same for the other 2 models but then with the 02 and 03 files.\n",
    "\n",
    "We will also make 3 models for the \"new betas\" called posterior_with_new_betas_01, posterior_with_new_betas_02 and posterior_with_new_betas_03. Their corresponding training and test data (present in the data map) are respectively: \n",
    "-\tX_train_new_betas_01 with X_test_new_betas_01\n",
    "-\tX_train_new_betas_02 with X_test_new_betas_02\n",
    "-\tX_train_new_betas_03 with X_test_new_betas_03\n",
    "\n",
    "Again as example: the model \"posterior_with_new_betas_01\" is trained with X_train_new_betas_01 (so you need to test this model with X_test_new_betas_01). Same for the other 2 models but then with the 02 and 03 files.\n",
    "\n",
    "The reason to train multiple models is to avoid that by chance a better train/test set is generated which can lead to optimistic/worse results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa87b568-4118-493c-b7e9-cba3db39b7a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11520, 12)\n",
      "[49.20419452  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.9181171   0.69864174  0.71237282  0.71968232]\n"
     ]
    }
   ],
   "source": [
    "# Change the line below to the desired X that you want to import.\n",
    "# X.npy in data map contains the \"old betas\" and X_with_new_betas.npy contains the \"new betas\".\n",
    "X = np.load(cwd_path + \"/data/X.npy\")\n",
    "print(X.shape)\n",
    "print(X[0])\n",
    "\n",
    "# # Split this matrix into train and test data.\n",
    "\n",
    "# # Shuffle the X to make sure different combinations end up in the train and test sets.\n",
    "# np.random.shuffle(X)\n",
    "\n",
    "# # Take 10% of the data for test (there are 11520 simulations in X, 10% is 1152 which goes to test, the rest goes to X_train).\n",
    "# X_train = X[:10368, :]\n",
    "# X_test = X[10368:, :]\n",
    "\n",
    "# # Save this train and test set; specify the files you want the train and test data to be saved to.\n",
    "# np.save(cwd_path + \"/data/X_train_[...].npy\", X_train)\n",
    "# np.save(cwd_path + \"/data/X_test_[...].npy\", X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4b9a8d-93ab-43c0-bdfc-50aca85e2187",
   "metadata": {},
   "source": [
    "## Training the Neural Network <a class=\"anchor\" id=\"training\"></a>\n",
    "\n",
    "There is a pretrained posterior present in the models map. If you want to retrain it, uncomment the block of code below before running it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4407948a-0310-474d-aa31-30de5523615c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the 2 lines below to load the desired train and test set (here I loaded X_train_01 and X_test_01)\n",
    "# which is with the \"old betas\" (first run from the triplo).\n",
    "X_train_01 = np.load(args.data, allow_pickle=True)\n",
    "X_test_01 = np.load(cwd_path + \"/data/X_test_01.npy\")\n",
    "\n",
    "# # Establish how many simulations are in the data \n",
    "# num_simulations = X_train_01.shape[0]\n",
    "\n",
    "# # Seperate simulation parameters and summary statistics\n",
    "# params = X_train_01[:, -4:]\n",
    "# stats  = X_train_01[:, :-4]\n",
    "\n",
    "# # When working with Torch, the matrix has to be parsed to a Torch object \n",
    "# theta = torch.from_numpy(params).float()\n",
    "# x = torch.from_numpy(stats).float()\n",
    "\n",
    "# # Train the posterior with all the arguments needed \n",
    "# posterior = train(num_simulations,\n",
    "#                     x,\n",
    "#                     theta,\n",
    "#                     num_threads         = args.num_threads,\n",
    "#                     method              = args.method,\n",
    "#                     device              = args.device,\n",
    "#                     density_estimator   = args.density_estimator\n",
    "#                     )\n",
    "\n",
    "# # Save posterior (intermediate result); specify the files you want the train and test data to be saved to.\n",
    "# torch.save(posterior, cwd_path + \"/models/posterior_[...].pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45087be-d8dc-4520-9ce7-5035813a611b",
   "metadata": {},
   "source": [
    "## Validating Results & Saving Plots <a class=\"anchor\" id=\"validation\"></a>\n",
    "\n",
    "*Warning: when this code is run 1152 pairplots and correlation plots will be saved in the png-map! (for every simulation 2 plots and we have 1152 simulations as test data).*\n",
    "\n",
    "To-do: change the for-loop to display the plot in a different way (don't use pairplots) & incorporate the error between the actual value and the inferred parameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ea7d90fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tests results:  test_suite_10-26_21_13_56\n",
      "Testing Parameters: \n",
      "\t Threshold: 0.3\n",
      "\t Tolerance: 1.0\n",
      "\t Activation Threshold: 10.0\n",
      "\t Number of Samples: 500\n"
     ]
    }
   ],
   "source": [
    "\n",
    "extensions = {\"posterior\": \".pt\", \"test\": \".npy\"}\n",
    "\n",
    "# posterior_prefix = \"posterior_0\"\n",
    "# posterior_prefix = \"posterior_with_new_betas_0\"\n",
    "posterior_prefix = \"posterior_with_all_data_\"\n",
    "\n",
    "# test_mat_prefix = \"X_test_0\"\n",
    "test_mat_prefix = \"all_data_test_\"\n",
    "\n",
    "posterior_models = []\n",
    "testing_data = []\n",
    "\n",
    "for root, dirs, files in os.walk(cwd_path + \"/models/\"):\n",
    "    for name in files:\n",
    "        if posterior_prefix in name:\n",
    "            posterior_models.append([name, torch.load(os.path.join(root, name))])\n",
    "\n",
    "for root, dirs, files in os.walk(cwd_path + \"/data/\"):\n",
    "    for name in files:\n",
    "        if test_mat_prefix in name:\n",
    "            testing_data.append([name , np.load(os.path.join(root, name))])\n",
    "            \n",
    "posterior_models = np.array(posterior_models, dtype=object)\n",
    "testing_data = np.array(testing_data, dtype=object)\n",
    "combi = np.array([[posterior_models[i],testing_data[i]] for i in range(len(testing_data))])\n",
    "test_results_arr = []\n",
    "\n",
    "\n",
    "run_tests = False\n",
    "\n",
    "threshold = 0.3\n",
    "tolerance = 1.0\n",
    "activation_threshold = 10.0\n",
    "num_samples = 500\n",
    "\n",
    "# cur_test_name = \"test_suite_\" + str(datetime.now())[5:10] + \"_\" + str(datetime.now())[11:-7].replace(\":\",\"_\")\n",
    "cur_test_name = \"test_suite_10-26_21_13_56\"\n",
    "\n",
    "if run_tests:\n",
    "    print(\"Running tests for models: \")\n",
    "    for posterior in posterior_models:\n",
    "        print(posterior[0])\n",
    "    print()\n",
    "    for (posterior, test_matrix) in combi:\n",
    "        test_results_arr.append(\n",
    "            test_posterior(\n",
    "                posterior[1], \n",
    "                test_matrix[1], \n",
    "                num_samples = num_samples, \n",
    "                threshold = threshold, \n",
    "                tolerance = tolerance,\n",
    "                activation_threshold = activation_threshold,\n",
    "                posterior_name = posterior[0])\n",
    "        )\n",
    "    np.save(os.path.join(cwd_path,\"test_results\",cur_test_name), test_results_arr)\n",
    "else:\n",
    "    print(\"Loading tests results: \", cur_test_name)\n",
    "    print(\"Testing Parameters: \")\n",
    "    with open(os.path.join(cwd_path,\"plot_images\",cur_test_name,\"test_params.txt\")) as f:\n",
    "        for line in f.readlines():\n",
    "            print(\"\\t\",line[:-1])\n",
    "    test_results_arr = np.load(os.path.join(cwd_path,\"test_results\",cur_test_name + \".npy\"), allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "01bcc93c-1841-4d8a-8885-c37f6cfcd2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_test_suite_dir = os.path.join(cwd_path, 'plot_images', cur_test_name)\n",
    "\n",
    "try:\n",
    "    os.makedirs(cur_test_suite_dir)\n",
    "except FileExistsError:\n",
    "    pass\n",
    "with open(os.path.join(cwd_path, 'plot_images', cur_test_suite_dir, 'test_params.txt'), 'w') as f:\n",
    "        f.write(\"Posterior Prefix: \" + posterior_prefix + \"\\n\")\n",
    "        f.write(\"Testing Prefix:   \" + test_mat_prefix + \"\\n\")\n",
    "        f.write(\"Threshold: \" + str(threshold) + \"\\n\")\n",
    "        f.write(\"Tolerance: \" + str(tolerance) + \"\\n\")\n",
    "        f.write(\"Activation Threshold: \" + str(activation_threshold) + \"\\n\")\n",
    "        f.write(\"Number of Samples: \" + str(num_samples) + \"\\n\")\n",
    "\n",
    "print(\"Saving plots...\")\n",
    "fig00 , _ = plot_average_accuracy(test_results_arr)\n",
    "fig00.savefig(os.path.join(cur_test_suite_dir, 'average_accuracy'))\n",
    "plt.close(fig00)\n",
    "for test_indx in tqdm(range(len(test_results_arr)), unit=\" model\", colour=\"green\", file=sys.stdout):\n",
    "    \n",
    "    test_result = test_results_arr[test_indx]\n",
    "    \n",
    "    cur_test_dir = os.path.join(cwd_path, 'plot_images', cur_test_suite_dir, os.path.splitext(combi[test_indx][0][0])[0])\n",
    "    \n",
    "    \n",
    "    \n",
    "    try:\n",
    "        os.mkdir(cur_test_dir)\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "    \n",
    "    fig1 = plot_confusion_matrices_and_balanced_accuracies(test_result)\n",
    "    fig2 = plot_confusion_matrices_and_balanced_accuracies_combinations(test_result)\n",
    "    fig3, _ = plot_layer_combinations(combi[test_indx][1][1])\n",
    "    \n",
    "    \n",
    "    fig1.savefig(os.path.join(cur_test_dir, 'accuracy'))  \n",
    "    fig2.savefig(os.path.join(cur_test_dir, 'accuracy_combinations'))  \n",
    "    fig3.savefig(os.path.join(cur_test_dir, 'activation_population_inputs'))\n",
    "    \n",
    "    plt.close(fig1)\n",
    "    plt.close(fig2)\n",
    "    plt.close(fig3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f44e9af-9747-43c1-a76a-54bb828aada8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:sbi_env]",
   "language": "python",
   "name": "conda-env-sbi_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
